{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T23:26:30.661439Z","iopub.status.busy":"2025-01-06T23:26:30.661211Z","iopub.status.idle":"2025-01-06T23:26:35.006686Z","shell.execute_reply":"2025-01-06T23:26:35.005994Z","shell.execute_reply.started":"2025-01-06T23:26:30.661418Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import os\n","import glob\n","import cv2\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from tqdm import tqdm\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["# Set up DataLoader"]},{"cell_type":"markdown","metadata":{},"source":["### Define text to int function"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T23:26:40.618959Z","iopub.status.busy":"2025-01-06T23:26:40.618598Z","iopub.status.idle":"2025-01-06T23:26:40.624040Z","shell.execute_reply":"2025-01-06T23:26:40.623067Z","shell.execute_reply.started":"2025-01-06T23:26:40.618931Z"},"trusted":true},"outputs":[],"source":["alphabet = \" ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n","blank_idx = 0\n","char2idx = {ch: i+1 for i, ch in enumerate(alphabet)} # Leave one blank token for CTC\n","idx2char = {v: k for k, v in char2idx.items()}\n","\n","def text_to_int_sequence(text, char2idx):\n","    text = text.upper()\n","    sequence = []\n","    for ch in text:\n","        if ch in char2idx:\n","            sequence.append(char2idx[ch])\n","        # else be blank\n","    return sequence"]},{"cell_type":"markdown","metadata":{},"source":["### Define dataset class"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T23:26:43.285509Z","iopub.status.busy":"2025-01-06T23:26:43.285178Z","iopub.status.idle":"2025-01-06T23:26:43.299445Z","shell.execute_reply":"2025-01-06T23:26:43.298555Z","shell.execute_reply.started":"2025-01-06T23:26:43.285481Z"},"trusted":true},"outputs":[],"source":["class BBCNewsVideoDataset(Dataset):\n","    \"\"\"\n","    A dataset that reads (video, transcript) pairs from 'pretrain' or 'main' directories.\n","    \n","    Each ID folder contains matching mp4/txt files named e.g. \"00001.mp4\" and \"00001.txt\".\n","    Parse the lines from .txt to extract the transcript text,\n","    and read frames from the corresponding .mp4 for the video data.\n","    \"\"\"\n","    def __init__(self, \n","                 root_dir,        # e.g. \"/kaggle/input/my_data\"\n","                 mode='pretrain', # or 'main'\n","                 transform=None,  # optional transforms on frames\n","                 max_frames=75):\n","        \"\"\"\n","        :param root_dir: path to the folder containing 'pretrain' and 'main' subdirs\n","        :param mode: which subdir to read from ('pretrain' or 'main')\n","        :param transform: optional torchvision transforms for the frames\n","        :param max_frames: if you want to limit frames per clip (just an example)\n","        \"\"\"\n","        super().__init__()\n","        self.root_dir = os.path.join(root_dir, mode)\n","        self.transform = transform\n","        self.max_frames = max_frames\n","        \n","        # Gather all mp4 files recursively\n","        # For example: root_dir/mode/*/*.mp4\n","        self.video_paths = sorted(glob.glob(os.path.join(self.root_dir, '*', '*.mp4')))\n","        \n","        # We'll derive the matching txt path by replacing .mp4 with .txt\n","        # or just see if it exists\n","        self.data = []\n","        for vp in self.video_paths:\n","            txt_path = vp.replace('.mp4', '.txt')\n","            if os.path.exists(txt_path):\n","                self.data.append((vp, txt_path))\n","            else:\n","                # If there's no matching txt, skip\n","                continue\n","                \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def parse_transcript(self, txt_path):\n","        \"\"\"\n","        Reads the .txt file to extract the transcript text after 'Text:' line.\n","        For your example, we ignore Conf, WORD lines, etc.\n","        \"\"\"\n","        transcript = \"\"\n","        with open(txt_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                line = line.strip()\n","                if line.startswith(\"Text:\"):\n","                    # Everything after \"Text:\" is the transcript\n","                    transcript = line.replace(\"Text:\", \"\").strip()\n","                    # remove trailing \"Conf:\" if it's on the same line (some are multiline)\n","                    if \"Conf:\" in transcript:\n","                        transcript = transcript.split(\"Conf:\")[0].strip()\n","                    break\n","        return transcript\n","    \n","    def read_video(self, video_path):\n","        \"\"\"\n","        Reads frames from mp4 using OpenCV (cv2) into a list of frames (H,W,3).\n","        Optionally limit to self.max_frames frames.\n","        \"\"\"\n","        frames = []\n","        cap = cv2.VideoCapture(video_path)\n","        frame_count = 0\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            # Convert BGR -> RGB\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frames.append(frame)\n","            frame_count += 1\n","            if self.max_frames is not None and frame_count >= self.max_frames:\n","                break\n","        cap.release()\n","        return frames\n","    \n","    def __getitem__(self, idx):\n","        video_path, txt_path = self.data[idx]\n","        \n","        # Parse transcript\n","        transcript = self.parse_transcript(txt_path)\n","        \n","        # Read frames\n","        frames = self.read_video(video_path)  # list of np arrays, shape (H, W, 3)\n","        \n","        # Apply any transform to each frame\n","        if self.transform:\n","            frames = [self.transform(img) for img in frames]\n","        else:\n","            # Convert frames to torch Tensors if no transform\n","            # shape => (C, H, W)\n","            frames = [torch.from_numpy(img).permute(2,0,1) for img in frames]\n","        \n","        # Stack into a single tensor => shape (T, C, H, W)\n","        # T = number of frames\n","        video_tensor = torch.stack(frames, dim=0).float()\n","\n","        # Return\n","        #   video_tensor: shape (T, 3, H, W)\n","        #   transcript: a string\n","        return video_tensor, transcript\n","\n","# For padding the videos to have the same number of frames and converting the chars into nums for ctc\n","def collate_fn_ctc(batch):\n","    \"\"\"\n","    batch: list of (frames, transcript_str)\n","        1) Convert transcript_str -> numeric\n","        2) Pad frames in time dimension\n","        3) Flatten targets\n","        4) Return (frames, targets, input_lengths, target_lengths)\n","    \"\"\"\n","    # Sort by descending frames length\n","    batch.sort(key=lambda x: x[0].shape[0], reverse=True)\n","\n","    frames_list, targets_list = [], []\n","    input_lengths, target_lengths = [], []\n","    max_len = 0\n","\n","    for (video_tensor, txt) in batch:\n","        T = video_tensor.shape[0]\n","        if T > max_len:\n","            max_len = T\n","        \n","    # Convert text -> numeric, build up final batch\n","    for (video_tensor, txt) in batch:\n","        frames_list.append(video_tensor)\n","        input_lengths.append(video_tensor.shape[0])\n","\n","        numeric_seq = text_to_int_sequence(txt, char2idx)\n","        target_lengths.append(len(numeric_seq))\n","        targets_list.append(torch.tensor(numeric_seq, dtype=torch.long))\n","\n","    # Pad frames to max_len\n","    padded_frames = []\n","    for vid in frames_list:\n","        T = vid.shape[0]\n","        if T < max_len:\n","            pad_amt = max_len - T\n","            vid = torch.nn.functional.pad(vid, (0,0,0,0,0,0,0,pad_amt))  # pad time dim\n","        padded_frames.append(vid)\n","    \n","    frames_tensor = torch.stack(padded_frames, dim=0)  # => (B, max_len, C, H, W)\n","    concat_targets = torch.cat(targets_list, dim=0)\n","    \n","    input_lengths = torch.tensor(input_lengths, dtype=torch.long)\n","    target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n","    \n","    return frames_tensor, concat_targets, input_lengths, target_lengths"]},{"cell_type":"markdown","metadata":{},"source":["### TEST Dataset Class"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T23:26:51.897064Z","iopub.status.busy":"2025-01-06T23:26:51.896738Z","iopub.status.idle":"2025-01-06T23:31:26.189676Z","shell.execute_reply":"2025-01-06T23:31:26.188791Z","shell.execute_reply.started":"2025-01-06T23:26:51.897039Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Pretrain dataset size: 96318\n","sample_video_tensor shape: torch.Size([75, 3, 50, 100])\n","sample_transcript: THESE DAYS WHEN YOU'RE COOKING CHIPS AT HOME THE TRADITIONAL CHIP PAN OFTEN STAYS ON THE SHELF IN FAVOUR OF A BAKING TRAY AND A BAG OF FROZEN OVEN\n","Batch video frames: 2\n","Batch transcript char length: 216\n","Batch frame nums: tensor([75, 75])\n","Batch transcript lengths: tensor([ 86, 130])\n"]}],"source":["root_dir = \"mvlrs_v1\"\n","\n","# Optional: define a transform for frames\n","# e.g. resize to (50,100) for LipNet style\n","transform = T.Compose([\n","    T.ToPILImage(),\n","    T.Resize((50,100)),  # (H, W) \n","    T.ToTensor()\n","])\n","\n","# Create a dataset for the 'pretrain' directory\n","pretrain_dataset = BBCNewsVideoDataset(root_dir, mode='pretrain', transform=transform)\n","print(\"Pretrain dataset size:\", len(pretrain_dataset))\n","\n","# Example item\n","sample_video_tensor, sample_transcript = pretrain_dataset[0]\n","print(\"sample_video_tensor shape:\", sample_video_tensor.shape)\n","print(\"sample_transcript:\", sample_transcript)\n","\n","# Create a DataLoader\n","pretrain_loader = DataLoader(pretrain_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn_ctc)\n","\n","# Example iteration\n","for batch in pretrain_loader:\n","    frames_tensor, concat_targets, input_lengths, target_lengths = batch  # but watch out, \"videos\" might be list of T x C x H x W\n","    print(\"Batch video frames:\", len(frames_tensor))\n","    print(\"Batch transcript char length:\", len(concat_targets))\n","    print(\"Batch frame nums:\", input_lengths)\n","    print(\"Batch transcript lengths:\", target_lengths)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["# Define Model Architecture"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T23:40:17.473319Z","iopub.status.busy":"2025-01-06T23:40:17.472949Z","iopub.status.idle":"2025-01-06T23:40:17.483020Z","shell.execute_reply":"2025-01-06T23:40:17.482219Z","shell.execute_reply.started":"2025-01-06T23:40:17.473293Z"},"trusted":true},"outputs":[],"source":["class LipNetPyTorch(nn.Module):\n","    def __init__(self,\n","                 img_c=3,\n","                 img_w=100,\n","                 img_h=50,\n","                 frames_n=75,\n","                 output_size=28,        # number of characters + 1 for blank\n","                 absolute_max_string_len=32  # not strictly needed in PyTorch model??\n","                 ):\n","        super(LipNetPyTorch, self).__init__()\n","        \n","        # 3D Conv block #1\n","        self.conv1 = nn.Conv3d(in_channels=img_c,\n","                               out_channels=32,\n","                               kernel_size=(3, 5, 5),\n","                               stride=(1, 2, 2),\n","                               padding=(1, 2, 2))\n","        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2),\n","                                  stride=(1, 2, 2))\n","        self.drop1 = nn.Dropout(0.5)\n","        \n","        # 3D Conv block #2\n","        self.conv2 = nn.Conv3d(in_channels=32,\n","                               out_channels=64,\n","                               kernel_size=(3, 5, 5),\n","                               stride=(1, 1, 1),\n","                               padding=(1, 2, 2))\n","        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2),\n","                                  stride=(1, 2, 2))\n","        self.drop2 = nn.Dropout(0.5)\n","        \n","        # 3D Conv block #3\n","        self.conv3 = nn.Conv3d(in_channels=64,\n","                               out_channels=96,\n","                               kernel_size=(3, 3, 3),\n","                               stride=(1, 1, 1),\n","                               padding=(1, 1, 1))\n","        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2),\n","                                  stride=(1, 2, 2))\n","        self.drop3 = nn.Dropout(0.5)\n","        \n","        # After these 3D convs, the shape in time dimension should remain ~frames_n\n","        # but height/width get downsampled heavily by stride/pool\n","        # We'll flatten spatial dims but keep the time dim for the RNN\n","\n","        # Dimensionality going into the GRUs:\n","        # original: (W=100, H=50)\n","        # conv1+pool1 => W -> (100/2/2)=25, H->(50/2/2)=12 (accounting for strides/pools)\n","        # conv2+pool2 => W -> 25/2=12,   H->12/2=6\n","        # conv3+pool3 => W -> 12/2=6,    H->6/2=3\n","        # => final is (N, 96, T, 3, 6)\n","        # => flattened per frame => 96*3*6 = 1728\n","\n","        self.gru_hidden_size = 256\n","        self.num_gru_layers = 2\n","        \n","        # Bi-directional GRU\n","        self.gru = nn.GRU(input_size=1728,\n","                          hidden_size=self.gru_hidden_size,\n","                          num_layers=self.num_gru_layers,\n","                          batch_first=True,\n","                          bidirectional=True)\n","        \n","        # Final linear layer to project onto output_size\n","        # Because it’s bidirectional, output size is 2 * gru_hidden_size\n","        self.fc = nn.Linear(self.gru_hidden_size * 2, output_size)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x shape expected as (batch, channels=3, frames=75, height=50, width=100)\n","        If your data is (batch, frames, height, width, channels),\n","        be sure to permute it before calling forward: x.permute(0,4,1,2,3)\n","        \"\"\"\n","        # (1) 3D conv/pool #1\n","        x = self.conv1(x)  # => (batch, 32, frames, H/2, W/2)\n","        x = F.relu(x)\n","        x = self.pool1(x)  # => (batch, 32, frames, H/4, W/4)\n","        x = self.drop1(x)\n","        \n","        # (2) 3D conv/pool #2\n","        x = self.conv2(x)  # => (batch, 64, frames, H/4, W/4)\n","        x = F.relu(x)\n","        x = self.pool2(x)  # => (batch, 64, frames, H/8, W/8)\n","        x = self.drop2(x)\n","        \n","        # (3) 3D conv/pool #3\n","        x = self.conv3(x)  # => (batch, 96, frames, H/8, W/8)\n","        x = F.relu(x)\n","        x = self.pool3(x)  # => (batch, 96, frames, H/16, W/16)\n","        x = self.drop3(x)\n","        \n","        # Now flatten the spatial dims but keep the time dim\n","        # x shape is (batch, 96, T, H’, W’)\n","        b, c, t, h, w = x.size()\n","        # -> (batch, t, c*h*w)\n","        x = x.permute(0, 2, 1, 3, 4)\n","        x = x.reshape(b, t, c*h*w)\n","        \n","        # (4) Bi-GRU\n","        # x => shape (batch, time, features)\n","        x, _ = self.gru(x)  # => (batch, time, 2*gru_hidden_size)\n","\n","        # (5) FC => output_size\n","        logits = self.fc(x)  # => (batch, time, output_size)\n","\n","        # For CTC, you’ll typically feed log_probs = F.log_softmax(logits, dim=2)\n","        return logits"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T02:03:36.983006Z","iopub.status.busy":"2025-01-06T02:03:36.982726Z","iopub.status.idle":"2025-01-06T02:03:37.338642Z","shell.execute_reply":"2025-01-06T02:03:37.337857Z","shell.execute_reply.started":"2025-01-06T02:03:36.982987Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Output shape: torch.Size([2, 75, 28])\n"]}],"source":["# quick test with a dummy input:\n","model = LipNetPyTorch()\n","# dummy input: batch=2, channels=3, frames=75, H=50, W=100\n","dummy_input = torch.randn(2, 3, 75, 50, 100)\n","out = model(dummy_input)\n","print(\"Output shape:\", out.shape)\n","# Should be (2, 75, 28)"]},{"cell_type":"markdown","metadata":{},"source":["# Define Train Loop"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T23:40:29.363832Z","iopub.status.busy":"2025-01-06T23:40:29.363507Z","iopub.status.idle":"2025-01-06T23:40:29.775574Z","shell.execute_reply":"2025-01-06T23:40:29.774711Z","shell.execute_reply.started":"2025-01-06T23:40:29.363805Z"},"trusted":true},"outputs":[],"source":["#   frames   => (batch, 3, T, 50, 100)   # videos\n","#   targets  => 1D Tensor of all targets concatenated\n","#   input_lengths => lengths of each sequence in frames\n","#   target_lengths => lengths of each transcription\n","\n","def train_step(frames, targets, input_lengths, target_lengths):\n","    frames = frames.cuda()\n","    targets = targets.cuda()\n","\n","    optimizer.zero_grad()\n","    \n","    # (batch, time, output_size)\n","    logits = model(frames)\n","    # PyTorch’s CTC wants => (time, batch, class)\n","    logits_for_ctc = logits.permute(1, 0, 2)  # => (T, N, C)\n","    \n","    # Compute log probs\n","    log_probs = F.log_softmax(logits_for_ctc, dim=2)\n","\n","    loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n","    loss.backward()\n","    optimizer.step()\n","    return loss.item()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-01-06T23:40:34.742797Z","iopub.status.busy":"2025-01-06T23:40:34.742471Z","iopub.status.idle":"2025-01-06T23:40:34.748180Z","shell.execute_reply":"2025-01-06T23:40:34.747200Z","shell.execute_reply.started":"2025-01-06T23:40:34.742772Z"},"trusted":true},"outputs":[],"source":["def train_model(model, train_loader, ctc_loss, optimizer, num_epochs=10, device='cuda'):\n","    \"\"\"\n","    Args:\n","        model: your LipNetPyTorch model\n","        train_loader: DataLoader yielding (frames, targets, input_lengths, target_lengths)\n","        ctc_loss: nn.CTCLoss (or similar)\n","        optimizer: e.g. torch.optim.Adam(model.parameters())\n","        num_epochs: total epochs to train\n","        device: 'cuda' or 'cpu'\n","    \"\"\"\n","    \n","    model.to(device)\n","    model.train()\n","    \n","    for epoch in range(num_epochs):\n","        epoch_start = time.time()\n","        total_loss = 0.0\n","        \n","        for batch_idx, (frames, targets, input_lengths, target_lengths) in enumerate(\n","            tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n","        ):\n","            # frames shape is (B, T, C, H, W). \n","            # need (B, C, T, H, W) for the model\n","            frames = frames.permute(0, 2, 1, 3, 4)  # => (B, C, T, H, W)\n","\n","            # Perform a single train step\n","            loss_value = train_step(frames, targets, input_lengths, target_lengths)\n","\n","            total_loss += loss_value\n","        \n","        avg_loss = total_loss / len(train_loader)\n","        epoch_time = time.time() - epoch_start\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] took {epoch_time:.2f} seconds, Loss: {avg_loss:.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"execution_failed":"2025-01-06T23:56:19.934Z","iopub.execute_input":"2025-01-06T23:51:34.703758Z","iopub.status.busy":"2025-01-06T23:51:34.703407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Main dataset size: 48165\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   4%|▍         | 65/1506 [01:53<42:06,  1.75s/batch]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMain dataset size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(main_dataset))\n\u001b[0;32m     20\u001b[0m main_loader \u001b[38;5;241m=\u001b[39m DataLoader(main_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn_ctc)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctc_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, ctc_loss, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     17\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# frames shape is (B, T, C, H, W). \u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# need (B, C, T, H, W) for the model\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# => (B, C, T, H, W)\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Perform a single train step\u001b[39;49;00m\n","File \u001b[1;32md:\\Projects\\Github\\lip-read\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n","File \u001b[1;32md:\\Projects\\Github\\lip-read\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n","File \u001b[1;32md:\\Projects\\Github\\lip-read\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32md:\\Projects\\Github\\lip-read\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[3], line 93\u001b[0m, in \u001b[0;36mBBCNewsVideoDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Apply any transform to each frame\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 93\u001b[0m     frames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m frames]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Convert frames to torch Tensors if no transform\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# shape => (C, H, W)\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     frames \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfrom_numpy(img)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m frames]\n","File \u001b[1;32md:\\Projects\\Github\\lip-read\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[1;32md:\\Projects\\Github\\lip-read\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Projects\\Github\\lip-read\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = LipNetPyTorch()\n","if torch.cuda.device_count() > 1:\n","    print(\"Using DataParallel on\", torch.cuda.device_count(), \"GPUs!\")\n","    model = torch.nn.DataParallel(model)\n","model = model.cuda()\n","ctc_loss = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Set up dataloader\n","root_dir = \"mvlrs_v1\"\n","\n","transform = T.Compose([\n","    T.ToPILImage(),\n","    T.Resize((50,100)),  # (H, W) \n","    T.ToTensor()\n","])\n","\n","main_dataset = BBCNewsVideoDataset(root_dir, mode='main', transform=transform)\n","print(\"Main dataset size:\", len(main_dataset))\n","main_loader = DataLoader(main_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn_ctc)\n","\n","train_model(model, main_loader, ctc_loss, optimizer, num_epochs=1, device='cuda')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6416682,"sourceId":10360824,"sourceType":"datasetVersion"}],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
