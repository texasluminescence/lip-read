{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T23:26:30.661439Z",
     "iopub.status.busy": "2025-01-06T23:26:30.661211Z",
     "iopub.status.idle": "2025-01-06T23:26:35.006686Z",
     "shell.execute_reply": "2025-01-06T23:26:35.005994Z",
     "shell.execute_reply.started": "2025-01-06T23:26:30.661418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define text to int function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T23:26:40.618959Z",
     "iopub.status.busy": "2025-01-06T23:26:40.618598Z",
     "iopub.status.idle": "2025-01-06T23:26:40.624040Z",
     "shell.execute_reply": "2025-01-06T23:26:40.623067Z",
     "shell.execute_reply.started": "2025-01-06T23:26:40.618931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "alphabet = \" ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "blank_idx = 0\n",
    "char2idx = {ch: i+1 for i, ch in enumerate(alphabet)} # Leave one blank token for CTC\n",
    "idx2char = {v: k for k, v in char2idx.items()}\n",
    "\n",
    "def text_to_int_sequence(text, char2idx):\n",
    "    text = text.upper()\n",
    "    sequence = []\n",
    "    for ch in text:\n",
    "        if ch in char2idx:\n",
    "            sequence.append(char2idx[ch])\n",
    "        # else be blank\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T23:26:43.285509Z",
     "iopub.status.busy": "2025-01-06T23:26:43.285178Z",
     "iopub.status.idle": "2025-01-06T23:26:43.299445Z",
     "shell.execute_reply": "2025-01-06T23:26:43.298555Z",
     "shell.execute_reply.started": "2025-01-06T23:26:43.285481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BBCNewsVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that reads (video, transcript) pairs from 'pretrain' or 'main' directories.\n",
    "    \n",
    "    Each ID folder contains matching mp4/txt files named e.g. \"00001.mp4\" and \"00001.txt\".\n",
    "    Parse the lines from .txt to extract the transcript text,\n",
    "    and read frames from the corresponding .mp4 for the video data.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 root_dir,        # e.g. \"/kaggle/input/my_data\"\n",
    "                 mode='pretrain', # or 'main'\n",
    "                 transform=None,  # optional transforms on frames\n",
    "                 max_frames=75):\n",
    "        \"\"\"\n",
    "        :param root_dir: path to the folder containing 'pretrain' and 'main' subdirs\n",
    "        :param mode: which subdir to read from ('pretrain' or 'main')\n",
    "        :param transform: optional torchvision transforms for the frames\n",
    "        :param max_frames: if you want to limit frames per clip (just an example)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root_dir = os.path.join(root_dir, mode)\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        # Gather all mp4 files recursively\n",
    "        # For example: root_dir/mode/*/*.mp4\n",
    "        self.video_paths = sorted(glob.glob(os.path.join(self.root_dir, '*', '*.mp4')))\n",
    "        \n",
    "        # We'll derive the matching txt path by replacing .mp4 with .txt\n",
    "        # or just see if it exists\n",
    "        self.data = []\n",
    "        for vp in self.video_paths:\n",
    "            txt_path = vp.replace('.mp4', '.txt')\n",
    "            if os.path.exists(txt_path):\n",
    "                self.data.append((vp, txt_path))\n",
    "            else:\n",
    "                # If there's no matching txt, skip\n",
    "                continue\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def parse_transcript(self, txt_path):\n",
    "        \"\"\"\n",
    "        Reads the .txt file to extract the transcript text after 'Text:' line.\n",
    "        For your example, we ignore Conf, WORD lines, etc.\n",
    "        \"\"\"\n",
    "        transcript = \"\"\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"Text:\"):\n",
    "                    # Everything after \"Text:\" is the transcript\n",
    "                    transcript = line.replace(\"Text:\", \"\").strip()\n",
    "                    # remove trailing \"Conf:\" if it's on the same line (some are multiline)\n",
    "                    if \"Conf:\" in transcript:\n",
    "                        transcript = transcript.split(\"Conf:\")[0].strip()\n",
    "                    break\n",
    "        return transcript\n",
    "    \n",
    "    def read_video(self, video_path):\n",
    "        \"\"\"\n",
    "        Reads frames from mp4 using OpenCV (cv2) into a list of frames (H,W,3).\n",
    "        Optionally limit to self.max_frames frames.\n",
    "        \"\"\"\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Convert BGR -> RGB\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "            frame_count += 1\n",
    "            if self.max_frames is not None and frame_count >= self.max_frames:\n",
    "                break\n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path, txt_path = self.data[idx]\n",
    "        \n",
    "        # Parse transcript\n",
    "        transcript = self.parse_transcript(txt_path)\n",
    "        \n",
    "        # Read frames\n",
    "        frames = self.read_video(video_path)  # list of np arrays, shape (H, W, 3)\n",
    "        \n",
    "        # Apply any transform to each frame\n",
    "        if self.transform:\n",
    "            frames = [self.transform(img) for img in frames]\n",
    "        else:\n",
    "            # Convert frames to torch Tensors if no transform\n",
    "            # shape => (C, H, W)\n",
    "            frames = [torch.from_numpy(img).permute(2,0,1) for img in frames]\n",
    "        \n",
    "        # Stack into a single tensor => shape (T, C, H, W)\n",
    "        # T = number of frames\n",
    "        video_tensor = torch.stack(frames, dim=0).float()\n",
    "\n",
    "        # Return\n",
    "        #   video_tensor: shape (T, 3, H, W)\n",
    "        #   transcript: a string\n",
    "        return video_tensor, transcript\n",
    "\n",
    "# For padding the videos to have the same number of frames and converting the chars into nums for ctc\n",
    "def collate_fn_ctc(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (frames, transcript_str)\n",
    "        1) Convert transcript_str -> numeric\n",
    "        2) Pad frames in time dimension\n",
    "        3) Flatten targets\n",
    "        4) Return (frames, targets, input_lengths, target_lengths)\n",
    "    \"\"\"\n",
    "    # Sort by descending frames length\n",
    "    batch.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "\n",
    "    frames_list, targets_list = [], []\n",
    "    input_lengths, target_lengths = [], []\n",
    "    max_len = 0\n",
    "\n",
    "    for (video_tensor, txt) in batch:\n",
    "        T = video_tensor.shape[0]\n",
    "        if T > max_len:\n",
    "            max_len = T\n",
    "        \n",
    "    # Convert text -> numeric, build up final batch\n",
    "    for (video_tensor, txt) in batch:\n",
    "        frames_list.append(video_tensor)\n",
    "        input_lengths.append(video_tensor.shape[0])\n",
    "\n",
    "        numeric_seq = text_to_int_sequence(txt, char2idx)\n",
    "        target_lengths.append(len(numeric_seq))\n",
    "        targets_list.append(torch.tensor(numeric_seq, dtype=torch.long))\n",
    "\n",
    "    # Pad frames to max_len\n",
    "    padded_frames = []\n",
    "    for vid in frames_list:\n",
    "        T = vid.shape[0]\n",
    "        if T < max_len:\n",
    "            pad_amt = max_len - T\n",
    "            vid = torch.nn.functional.pad(vid, (0,0,0,0,0,0,0,pad_amt))  # pad time dim\n",
    "        padded_frames.append(vid)\n",
    "    \n",
    "    frames_tensor = torch.stack(padded_frames, dim=0)  # => (B, max_len, C, H, W)\n",
    "    concat_targets = torch.cat(targets_list, dim=0)\n",
    "    \n",
    "    input_lengths = torch.tensor(input_lengths, dtype=torch.long)\n",
    "    target_lengths = torch.tensor(target_lengths, dtype=torch.long)\n",
    "    \n",
    "    return frames_tensor, concat_targets, input_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T23:26:51.897064Z",
     "iopub.status.busy": "2025-01-06T23:26:51.896738Z",
     "iopub.status.idle": "2025-01-06T23:31:26.189676Z",
     "shell.execute_reply": "2025-01-06T23:31:26.188791Z",
     "shell.execute_reply.started": "2025-01-06T23:26:51.897039Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain dataset size: 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrain dataset size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(pretrain_dataset))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Example item\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m sample_video_tensor, sample_transcript \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_video_tensor shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_video_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_transcript:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_transcript)\n",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m, in \u001b[0;36mBBCNewsVideoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 83\u001b[0m     video_path, txt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Parse transcript\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     transcript \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_transcript(txt_path)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "root_dir = \"data/mvlrs_v1\"\n",
    "\n",
    "# Optional: define a transform for frames\n",
    "# e.g. resize to (50,100) for LipNet style\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((50,100)),  # (H, W) \n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Create a dataset for the 'pretrain' directory\n",
    "pretrain_dataset = BBCNewsVideoDataset(root_dir, mode='pretrain', transform=transform)\n",
    "print(\"Pretrain dataset size:\", len(pretrain_dataset))\n",
    "\n",
    "# Example item\n",
    "sample_video_tensor, sample_transcript = pretrain_dataset[0]\n",
    "print(\"sample_video_tensor shape:\", sample_video_tensor.shape)\n",
    "print(\"sample_transcript:\", sample_transcript)\n",
    "\n",
    "# Create a DataLoader\n",
    "pretrain_loader = DataLoader(pretrain_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn_ctc)\n",
    "\n",
    "# Example iteration\n",
    "for batch in pretrain_loader:\n",
    "    frames_tensor, concat_targets, input_lengths, target_lengths = batch  # but watch out, \"videos\" might be list of T x C x H x W\n",
    "    print(\"Batch video frames:\", len(frames_tensor))\n",
    "    print(\"Batch transcript char length:\", len(concat_targets))\n",
    "    print(\"Batch frame nums:\", input_lengths)\n",
    "    print(\"Batch transcript lengths:\", target_lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T23:40:17.473319Z",
     "iopub.status.busy": "2025-01-06T23:40:17.472949Z",
     "iopub.status.idle": "2025-01-06T23:40:17.483020Z",
     "shell.execute_reply": "2025-01-06T23:40:17.482219Z",
     "shell.execute_reply.started": "2025-01-06T23:40:17.473293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LipNetPyTorch(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_c=3,\n",
    "                 img_w=100,\n",
    "                 img_h=50,\n",
    "                 frames_n=75,\n",
    "                 output_size=28,        # number of characters + 1 for blank\n",
    "                 absolute_max_string_len=32  # not strictly needed in PyTorch model??\n",
    "                 ):\n",
    "        super(LipNetPyTorch, self).__init__()\n",
    "        \n",
    "        # 3D Conv block #1\n",
    "        self.conv1 = nn.Conv3d(in_channels=img_c,\n",
    "                               out_channels=32,\n",
    "                               kernel_size=(3, 5, 5),\n",
    "                               stride=(1, 2, 2),\n",
    "                               padding=(1, 2, 2))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2),\n",
    "                                  stride=(1, 2, 2))\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        # 3D Conv block #2\n",
    "        self.conv2 = nn.Conv3d(in_channels=32,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=(3, 5, 5),\n",
    "                               stride=(1, 1, 1),\n",
    "                               padding=(1, 2, 2))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2),\n",
    "                                  stride=(1, 2, 2))\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        # 3D Conv block #3\n",
    "        self.conv3 = nn.Conv3d(in_channels=64,\n",
    "                               out_channels=96,\n",
    "                               kernel_size=(3, 3, 3),\n",
    "                               stride=(1, 1, 1),\n",
    "                               padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2),\n",
    "                                  stride=(1, 2, 2))\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        \n",
    "        # After these 3D convs, the shape in time dimension should remain ~frames_n\n",
    "        # but height/width get downsampled heavily by stride/pool\n",
    "        # We'll flatten spatial dims but keep the time dim for the RNN\n",
    "\n",
    "        # Dimensionality going into the GRUs:\n",
    "        # original: (W=100, H=50)\n",
    "        # conv1+pool1 => W -> (100/2/2)=25, H->(50/2/2)=12 (accounting for strides/pools)\n",
    "        # conv2+pool2 => W -> 25/2=12,   H->12/2=6\n",
    "        # conv3+pool3 => W -> 12/2=6,    H->6/2=3\n",
    "        # => final is (N, 96, T, 3, 6)\n",
    "        # => flattened per frame => 96*3*6 = 1728\n",
    "\n",
    "        self.gru_hidden_size = 256\n",
    "        self.num_gru_layers = 2\n",
    "        \n",
    "        # Bi-directional GRU\n",
    "        self.gru = nn.GRU(input_size=1728,\n",
    "                          hidden_size=self.gru_hidden_size,\n",
    "                          num_layers=self.num_gru_layers,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True)\n",
    "        \n",
    "        # Final linear layer to project onto output_size\n",
    "        # Because it’s bidirectional, output size is 2 * gru_hidden_size\n",
    "        self.fc = nn.Linear(self.gru_hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape expected as (batch, channels=3, frames=75, height=50, width=100)\n",
    "        If your data is (batch, frames, height, width, channels),\n",
    "        be sure to permute it before calling forward: x.permute(0,4,1,2,3)\n",
    "        \"\"\"\n",
    "        # (1) 3D conv/pool #1\n",
    "        x = self.conv1(x)  # => (batch, 32, frames, H/2, W/2)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)  # => (batch, 32, frames, H/4, W/4)\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        # (2) 3D conv/pool #2\n",
    "        x = self.conv2(x)  # => (batch, 64, frames, H/4, W/4)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)  # => (batch, 64, frames, H/8, W/8)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        # (3) 3D conv/pool #3\n",
    "        x = self.conv3(x)  # => (batch, 96, frames, H/8, W/8)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)  # => (batch, 96, frames, H/16, W/16)\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        # Now flatten the spatial dims but keep the time dim\n",
    "        # x shape is (batch, 96, T, H’, W’)\n",
    "        b, c, t, h, w = x.size()\n",
    "        # -> (batch, t, c*h*w)\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = x.reshape(b, t, c*h*w)\n",
    "        \n",
    "        # (4) Bi-GRU\n",
    "        # x => shape (batch, time, features)\n",
    "        x, _ = self.gru(x)  # => (batch, time, 2*gru_hidden_size)\n",
    "\n",
    "        # (5) FC => output_size\n",
    "        logits = self.fc(x)  # => (batch, time, output_size)\n",
    "\n",
    "        # For CTC, you’ll typically feed log_probs = F.log_softmax(logits, dim=2)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T02:03:36.983006Z",
     "iopub.status.busy": "2025-01-06T02:03:36.982726Z",
     "iopub.status.idle": "2025-01-06T02:03:37.338642Z",
     "shell.execute_reply": "2025-01-06T02:03:37.337857Z",
     "shell.execute_reply.started": "2025-01-06T02:03:36.982987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# quick test with a dummy input:\n",
    "model = LipNetPyTorch()\n",
    "# dummy input: batch=2, channels=3, frames=75, H=50, W=100\n",
    "dummy_input = torch.randn(2, 3, 75, 50, 100)\n",
    "out = model(dummy_input)\n",
    "print(\"Output shape:\", out.shape)\n",
    "# Should be (2, 75, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T23:40:29.363832Z",
     "iopub.status.busy": "2025-01-06T23:40:29.363507Z",
     "iopub.status.idle": "2025-01-06T23:40:29.775574Z",
     "shell.execute_reply": "2025-01-06T23:40:29.774711Z",
     "shell.execute_reply.started": "2025-01-06T23:40:29.363805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#   frames   => (batch, 3, T, 50, 100)   # videos\n",
    "#   targets  => 1D Tensor of all targets concatenated\n",
    "#   input_lengths => lengths of each sequence in frames\n",
    "#   target_lengths => lengths of each transcription\n",
    "\n",
    "def train_step(frames, targets, input_lengths, target_lengths):\n",
    "    frames = frames.cuda()\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # (batch, time, output_size)\n",
    "    logits = model(frames)\n",
    "    # PyTorch’s CTC wants => (time, batch, class)\n",
    "    logits_for_ctc = logits.permute(1, 0, 2)  # => (T, N, C)\n",
    "    \n",
    "    # Compute log probs\n",
    "    log_probs = F.log_softmax(logits_for_ctc, dim=2)\n",
    "\n",
    "    loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T23:40:34.742797Z",
     "iopub.status.busy": "2025-01-06T23:40:34.742471Z",
     "iopub.status.idle": "2025-01-06T23:40:34.748180Z",
     "shell.execute_reply": "2025-01-06T23:40:34.747200Z",
     "shell.execute_reply.started": "2025-01-06T23:40:34.742772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, ctc_loss, optimizer, num_epochs=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: your LipNetPyTorch model\n",
    "        train_loader: DataLoader yielding (frames, targets, input_lengths, target_lengths)\n",
    "        ctc_loss: nn.CTCLoss (or similar)\n",
    "        optimizer: e.g. torch.optim.Adam(model.parameters())\n",
    "        num_epochs: total epochs to train\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \"\"\"\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (frames, targets, input_lengths, target_lengths) in enumerate(\n",
    "            tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "        ):\n",
    "            # frames shape is (B, T, C, H, W). \n",
    "            # need (B, C, T, H, W) for the model\n",
    "            frames = frames.permute(0, 2, 1, 3, 4)  # => (B, C, T, H, W)\n",
    "\n",
    "            # Perform a single train step\n",
    "            loss_value = train_step(frames, targets, input_lengths, target_lengths)\n",
    "\n",
    "            total_loss += loss_value\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] took {epoch_time:.2f} seconds, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-06T23:56:19.934Z",
     "iopub.execute_input": "2025-01-06T23:51:34.703758Z",
     "iopub.status.busy": "2025-01-06T23:51:34.703407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = LipNetPyTorch()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using DataParallel on\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "ctc_loss = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set up dataloader\n",
    "root_dir = \"mvlrs_v1\"\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((50,100)),  # (H, W) \n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "main_dataset = BBCNewsVideoDataset(root_dir, mode='main', transform=transform)\n",
    "print(\"Main dataset size:\", len(main_dataset))\n",
    "main_loader = DataLoader(main_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn_ctc)\n",
    "\n",
    "train_model(model, main_loader, ctc_loss, optimizer, num_epochs=1, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6416682,
     "sourceId": 10360824,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "lip-read",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
